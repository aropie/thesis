\section{Stochastic gradient descent}
Si se considera el caso en que se tiene un \sout{very large dataset} con millones
de puntos con datos, correr un entrenamiento con GRADIENT BATCH DESCENT puede ser
un proceso sumamente costoso computacionalmente ya que se requiere reevaluar
todo el DATASET cada vez que se toma un \textit{paso} hacia el mínimo global.

Una alternativa popular al algoritmo BATCH GRADIENT DESCENT es \textit{STOCHASTIC
  GRADIENT DESCENT}, llamado también GRADIENT DESCENT \textit{iterativo}. En lugar
de actualizar los pesos basado en la suma de los errores acumulados de todas las
muestras $x^{(i)}$:
\begin{equation}
  \Delta w = \eta \sum_i(y^{(i)} - \phi(z^{(i)}))x^{(i)}
\end{equation}

Se actualizan los datos de manera incremental para cada muestra del entrenamiento:
\begin{equation}
  \eta(y^{(i)} - \phi(z^{(i)}))x^{(i)}
\end{equation}

Aunque el STOCHASTIC GRADIENT DESCENT podria ser considerado una aproximación
del GRADIENT DESCENT, por lo general converge mucho más rápido debido a las
actualizaciones tan frecuentes de los pesos. Como cada gradiente se calcula
basado en un sólo ejemplo de entrenamiento, \sout{the error surface is noisier than in gradient descent, which can also have
the advantage that stochastic gradient descent can escape shallow local minima more
readily}. Para obtener resultados \sout{precisos} con \sout{stochastic gradient descent}
es importante que se tomen los datos de forma aleatoria.

Otra ventaja del \sout{stochastic gradient descent} es que se puede usar para
hacer \textit{aprendizaje en línea}. Esto quiere decir que el modelo es entrenado
\sout{on the fly} al momento mientras más y más datos van llegando. Esto es
especialmente útil cuando se están acumulando grandes cantidades de datos.
Usando entrenamiento en línea, el sistema puede adaptarse inmediatamente a
los cambios y los datos de entrenamiento pueden ser descartados después de
actualizar el modelo, si el espacio de almacenamiento fuera un problema.

